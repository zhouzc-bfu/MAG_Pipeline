一、SRA原始数据下载（aspera高速下载）
1.下载aspera安装包到本地并上传至服务器

2.安装aspera
bash aspera-connect-*.sh
echo 'export PATH=~/.aspera/connect/bin:$PATH' >> ~/.bashrc && source ~/.bashrc

3.验证是否安装成功
ascp --version

4.获取aspera下载密码（高版本需要）
strings ~/.aspera/connect/bin/asperaconnect.bin | grep -Ee '^7[0-9a-f]{7}-'
#输出的第2行为密码

5.获取SRA数据下载地址
# 登录服务器终端执行
wget http://sxygptcloud.com:4000/enaBrowserTools-master.zip # 该链接为生信院自建镜像，其他用户另找链接下载即可
unzip enaBrowserTools-master.zip
cd enaBrowserTools-master/python3/
python enaDataGet.py -f fastq SRR20330029 #SRR20330029修改为你想要查询的SRA数据号
#获得下载地址，例如：vol1/fastq/SRR203/029/SRR20330029/
#该步也可使用ENA浏览器查询，网址：https://www.ebi.ac.uk/ena/browser/home

6.开始下载SRA数据
# 修改SRR20330029为自己需要的SRA号即可，修改下载地址的前缀SRR203/029（前缀为第4步获得）
ascp -v -Q -T -l 500m -P 33001 -k 1 -i ~/.aspera/connect/etc/aspera_tokenauth_id_rsa era-fasp@fasp.sra.ebi.ac.uk:vol1/fastq/SRR203/029/SRR20330029/ .
# ~/.aspera/connect/etc/aspera_tokenauth_id_rsa为密钥文件地址，不同版本该地址可能不一样，旧版本可能为~/.aspera/connect/etc/asperaweb_id_dsa.openssh，即：
ascp -v -Q -T -l 500m -P 33001 -k 1 -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh era-fasp@fasp.sra.ebi.ac.uk:vol1/fastq/SRR203/029/SRR20330029/ .  #-k后面可以使用3，即完全比较，耗时较长但是准确

# 弹出填写密码即把第4步所获密码复制进去即可

二、fastp(v1.0.1)数据质控
1.安装fastp
# bioconda可以安装，但一般不是最新版，推荐采用wget安装（fastp的Github官网有教程，如下）
# download the latest build
wget http://opengene.org/fastp/fastp
chmod a+x ./fastp

2.进入序列文件夹，将SRR子文件中的fastq.gz全部转移至父文件夹（若采用aspera下载需要此步，不生成SRR子文件夹的不需要此步）
#移动到当前目录
find . -name "*.fastq.gz" -exec mv {} . \;
#删除空目录
find . -type d -name "SRR*" -empty -delete

2.nano编写fastp质控脚本：
#!/bin/bash

# 创建输出目录
mkdir -p fastp_out

# 获取所有R1文件并提取样本名
for r1_file in *_1.fastq.gz; do
    # 提取样本名（去掉_1.fastq.gz）
    sample=${r1_file%_1.fastq.gz}
    
    # 对应的R2文件
    r2_file="${sample}_2.fastq.gz"
    
    echo "Processing $sample..."
    echo "R1: $r1_file, R2: $r2_file"
    
    if [[ -f "$r2_file" ]]; then
        fastp \
            -i "$r1_file" \
            -I "$r2_file" \
            -o "fastp_out/${sample}_1.clean.fastq.gz" \
            -O "fastp_out/${sample}_2.clean.fastq.gz" \
            -W 4 -M 20 -q 15 -u 40 -3 -n 5 -c -l 50 -w 8 \
            -h "fastp_out/${sample}.html" \
            -j "fastp_out/${sample}.json"
    else
        echo "Warning: R2 file $r2_file not found for sample $sample"
    fi
done

3.运行脚本
bash fastp.sh

三、Bowtie2(v2.5.4)去宿主
1.安装bowtie2
#下载 Bowtie2 2.5.4 官方二进制（替换所需版本号）
wget https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.5.4/bowtie2-2.5.4-linux-x86_64.zip
unzip bowtie2-2.5.4-linux-x86_64.zip

#添加到PATH（这里选择永久生效）
nano ~/.bashrc

#在文件末尾添加（full/path/to替换为文件原始路径）：
export PATH=/full/path/to/bowtie2-2.5.4-linux-x86_64:$PATH

#让配置立即生效
source ~/.bashrc

#验证是否安装成功
bowtie2 --version

2. 下载宿主的参考基因组数据（以野骆驼参考基因组GCF_009834535.1为例）
#创建目录存放基因组
mkdir -p ~/genomes/camel
cd ~/genomes/camel

#下载 RefSeq assembly 的 fasta（全染色体/contig）
wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/009/834/535/GCF_009834535.1_BCGSAC_Cfer_1.0/GCF_009834535.1_BCGSAC_Cfer_1.0_genomic.fna.gz

#解压
gunzip GCF_009834535.1_BCGSAC_Cfer_1.0_genomic.fna.gz

#下载完成后会得到：
~/genomes/camel/GCF_009834535.1_BCGSAC_Cfer_1.0_genomic.fna

3.建立Bowtie2索引文件
#在参考基因组所在文件夹运行：
bowtie2-build --threads 35 GCF_009834535.1_BCGSAC_Cfer_1.0_genomic.fna camel_index   #35线程加速

4.用 Bowtie2 比对宏基因组数据
#在数据所在文件夹写批量处理脚本
nano host_remove.sh

##脚本内容##
————————————————————————————————————————————————
#!/bin/bash

# 记录开始时间
echo "Start time: $(date)"

# 自动批量去宿主脚本 (Bowtie2)
# 宿主：Donkey

# Donkey宿主索引前缀（一定要和bt2文件前缀一致）
INDEX=/data/gxa3001s19/project/1_MAG_desert/analysis/2_host.remove_bowtie2/host_genomes/donkey/donkey_index

# CPU 核心数
THREADS=8

# 自动识别当前文件夹中所有 _1.clean.fastq.gz
for f in *_1.clean.fastq.gz
do
    sample=${f%%_1.clean.fastq.gz}
    R1=${sample}_1.clean.fastq.gz
    R2=${sample}_2.clean.fastq.gz

    echo ">>> Processing $sample ..."
    echo "    R1 = $R1"
    echo "    R2 = $R2"

    bowtie2 -x $INDEX \
        -1 $R1 -2 $R2 \
        --very-sensitive -p $THREADS \
        --un-conc-gz ${sample}_%.nohost.clean.fastq.gz \
        2> ${sample}_bowtie2.log

    echo ">>> Done: $sample"
    echo
done

# 记录结束时间
echo "End time: $(date)"
—————————————————————————————————————————————————
#运行脚本
bash host_remove.sh

四、Megahit(v1.2.9)组装
1.安装Megahit
conda install -c bioconda megahit

2.运行Megahit对去宿主后序列进行组装（示例单样本组装）
megahit -1 SRR13390162_1.nohost.clean.fastq.gz -2 SRR13390162_2.nohost.clean.fastq.gz -o ./SRR13390162_megahit_out -t 20 --min-contig-len 1000
# 批量for循环运行命令：
for r1 in *_1.nohost.clean.fastq.gz; do prefix="${r1%_1.nohost.clean.fastq.gz}"; megahit -1 "$r1" -2 "${prefix}_2.nohost.clean.fastq.gz" -o "${prefix}_megahit_out" -t 20 --min-contig-len 1000; done
# 批量删除所有样本的 intermediate_contigs 文件夹（为中间文件，删除节省存储空间）：
rm -r *_megahit_out/intermediate_contigs

五、MetaBAT2(v2.18)分箱
# 在单样本分箱中MetaBAT2具有最佳性能，参考文献：10.1016/j.cell.2019.01.001（Cell）→10.1186/s40168-018-0541-1（Microbiome）
# 流程可参考：https://blog.csdn.net/Asa12138/article/details/139424386
1.用 Bowtie2 构建 contig 索引
bowtie2-build final.contigs.fasta contigs_index

2.将 clean reads 比对到 contig
bowtie2 -x contigs_index -1 reads_1.fastq.gz -2 reads_2.fastq.gz -S ${samplename}.sam -p 20
# 注意：reads文件后缀必须统一成*_1.fastq.gz/*_2.fastq.gz格式（fastq扩展名的fq也可以）
# 改名命令：
rename -n 's/_1\.nohost\.clean\.fastq\.gz$/\.nohost\.clean_1.fastq.gz/' *_1.nohost.clean.fastq.gz #先预览
rename 's/_1\.nohost\.clean\.fastq\.gz$/\.nohost\.clean_1.fastq.gz/' *_1.nohost.clean.fastq.gz #确认后执行
rename -n 's/_2\.nohost\.clean\.fastq\.gz$/\.nohost\.clean_2.fastq.gz/' *_2.nohost.clean.fastq.gz #先预览
rename 's/_2\.nohost\.clean\.fastq\.gz$/\.nohost\.clean_2.fastq.gz/' *_2.nohost.clean.fastq.gz #确认后执行


3.SAM 转 BAM 并排序、建索引 # samtoos版本：v1.21
samtools view -bS ${samplename}.sam > ${samplename}.bam
samtools sort ${samplename}.bam -o ${samplename}.sorted.bam
samtools index ${samplename}.sorted.bam

4.计算 contig 覆盖度（MetaBAT2 需要）
MetaBAT2 官方提供 jgi_summarize_bam_contig_depths：
jgi_summarize_bam_contig_depths --outputDepth ${samplename}.depth.txt ${samplename}.sorted.bam

5.运行 MetaBAT2 进行分箱(选择最小contigs长度为1500)
metabat2 -i final.contigs.fasta -a ${samplename}.depth.txt -o ${samplename}_bins/bin -m 1500 -t 20

## 对某物种下所有样本进行自动批量分箱脚本见“批量binning脚本”

六、CheckM(v1.2.4)评估bin质量
1.参考官网安装CheckM及依赖项：https://github.com/Ecogenomics/CheckM/wiki

2.使用 lineage_wf 工作流程来评估基因组分箱
checkm lineage_wf -x fa ${samplename}_bins/ ${samplename}_checkm_out/ -t 20

3.导出结果报告
checkm qa ${samplename}_checkm_out/lineage.ms ${samplename}_checkm_out/ -o 2 > ${samplename}checkm_report.txt

## CheckM2(v1.1.0)评估bin质量（该步与CheckM可选，CheckM2采用的是机器学习的方式）
1.安装CheckM2
参考官网：https://github.com/chklovski/CheckM2

2.运行checkm2进行分箱质量评估
checkm2 predict --threads 20 --input ${samplename}_bins --output-directory ${samplename}_checkm2_out -x fa

3.查看结果报告
# 结果报告为quality_report.tsv，为方便阅读，我们按照Completeness（第2列）排序查看:
(head -1 quality_report.tsv && tail -n +2 quality_report.tsv | sort -t$'\t' -k2,2nr) | less

七、合并所有样本的bins进行CheckM质量评估，以及后续dRep去冗余处理
#合并脚本见“所有样本bins批量改名合并脚本”
